{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                         (I, shall, be, late, !, ')  Carroll"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undoubtedly</th>\n",
       "      <th>shoe</th>\n",
       "      <th>comprehend</th>\n",
       "      <th>heart</th>\n",
       "      <th>insult</th>\n",
       "      <th>convenient</th>\n",
       "      <th>benwick</th>\n",
       "      <th>chimney</th>\n",
       "      <th>stolen</th>\n",
       "      <th>belmont</th>\n",
       "      <th>...</th>\n",
       "      <th>distance</th>\n",
       "      <th>mere</th>\n",
       "      <th>performance</th>\n",
       "      <th>i'll</th>\n",
       "      <th>Norman</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>byron</th>\n",
       "      <th>husband</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  undoubtedly shoe comprehend heart insult convenient benwick chimney stolen  \\\n",
       "0           0    0          0     0      0          0       0       0      0   \n",
       "1           0    0          0     0      0          0       0       0      0   \n",
       "2           0    0          0     0      0          0       0       0      0   \n",
       "3           0    0          0     0      0          0       0       0      0   \n",
       "4           0    0          0     0      0          0       0       0      0   \n",
       "\n",
       "  belmont     ...     distance mere performance i'll Norman afternoon byron  \\\n",
       "0       0     ...            0    0           0    0      0         0     0   \n",
       "1       0     ...            0    0           0    0      0         0     0   \n",
       "2       0     ...            0    0           0    0      0         0     0   \n",
       "3       0     ...            0    0           0    0      0         0     0   \n",
       "4       0     ...            0    0           0    0      0         0     0   \n",
       "\n",
       "  husband                                      text_sentence text_source  \n",
       "0       0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1       0  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2       0  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3       0                                      (Oh, dear, !)     Carroll  \n",
       "4       0                         (I, shall, be, late, !, ')     Carroll  \n",
       "\n",
       "[5 rows x 3004 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.682445141066\n",
      "\n",
      "Test set score: 0.691729323308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "svc = SVC()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = svc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', svc.score(X_train, y_train))\n",
    "print('\\nTest set score:', svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add POS to BoW Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CCONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PART',\n",
       " 'PRON',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'VERB'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_pos = []\n",
    "for token in nlp(word_counts['text_sentence'][0].string):\n",
    "    list_pos.append(token.pos_)\n",
    "list_pos = set(list_pos)\n",
    "list_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_pos(sent, pos):\n",
    "    sentenc = nlp(sent.string)\n",
    "    i = 0\n",
    "    for token in sentenc:\n",
    "        if token.pos_ == pos:\n",
    "            i+=1\n",
    "    return i\n",
    "\n",
    "def speech_part(sent, part):\n",
    "    print('doing {}s'.format(part))\n",
    "    return [count_pos(word_counts['text_sentence'][i], part)\n",
    "        for i in range (len(word_counts['text_sentence']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing ADJs\n",
      "doing ADPs\n",
      "doing ADVs\n",
      "doing CCONJs\n",
      "doing DETs\n",
      "doing NOUNs\n",
      "doing PARTs\n",
      "doing PRONs\n",
      "doing VERBs\n"
     ]
    }
   ],
   "source": [
    "adj_count = speech_part(word_counts['text_sentence'], 'ADJ')\n",
    "adp_count = speech_part(word_counts['text_sentence'], 'ADP')\n",
    "adv_count = speech_part(word_counts['text_sentence'], 'ADV')\n",
    "cconj_count = speech_part(word_counts['text_sentence'], 'CCONJ')\n",
    "det_count = speech_part(word_counts['text_sentence'], 'DET')\n",
    "noun_count = speech_part(word_counts['text_sentence'], 'NOUN')\n",
    "part_count = speech_part(word_counts['text_sentence'], 'PART')\n",
    "pron_count = speech_part(word_counts['text_sentence'], 'PRON')\n",
    "verb_count = speech_part(word_counts['text_sentence'], 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features['adj_count']=adj_count\n",
    "features['adp_count']=adp_count\n",
    "features['adv_count']=adv_count\n",
    "features['cconj_count']=cconj_count\n",
    "features['det_count']=det_count\n",
    "features['noun_count']=noun_count\n",
    "features['part_count']=part_count\n",
    "features['pron_count']=pron_count\n",
    "features['verb_count']=verb_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_word_len = [len(word_counts['text_sentence'][i].string) / \n",
    "            len(word_counts['text_sentence'][i].string.split())\n",
    "                for i in range (len(word_counts['text_sentence']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['avg_wrd_len'] = avg_word_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrdspersent = [len(word_counts['text_sentence'][i].string.split()) \n",
    "               for i in range (len(word_counts['text_sentence']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features['avg_sent_len'] = wrdspersent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_ents = []\n",
    "for i in range (0, 5317):\n",
    "    sent_ents.append(nlp(word_counts['text_sentence'][i].string).ents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ent_labels = []\n",
    "for i in range (0, 5317):\n",
    "    for entity in sent_ents[i]:\n",
    "        ent_labels.append(entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CARDINAL',\n",
       " 'DATE',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LOC',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_entlabels = set(ent_labels)\n",
    "set_entlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_entlabels = pd.DataFrame(columns=set_entlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entity_lab(sent, ent):\n",
    "    print('doing {}s'.format(ent))\n",
    "    return [count_ent(word_counts['text_sentence'][i], ent)\n",
    "        for i in range (len(word_counts['text_sentence']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_ent(sent, ent):\n",
    "    sentent = nlp(sent.string).ents\n",
    "    i = 0\n",
    "    for entity in sentent:\n",
    "        if entity.label_ == ent:\n",
    "            i+=1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing CARDINALs\n",
      "doing DATEs\n",
      "doing FACs\n",
      "doing GPEs\n",
      "doing LANGUAGEs\n",
      "doing LOCs\n",
      "doing NORPs\n",
      "doing ORDINALs\n",
      "doing ORGs\n",
      "doing PERSONs\n",
      "doing PRODUCTs\n",
      "doing QUANTITYs\n",
      "doing TIMEs\n",
      "doing WORK_OF_ARTs\n"
     ]
    }
   ],
   "source": [
    "cardinal_count = entity_lab(word_counts['text_sentence'], 'CARDINAL')\n",
    "date_count = entity_lab(word_counts['text_sentence'], 'DATE')\n",
    "fac_count = entity_lab(word_counts['text_sentence'], 'FAC')\n",
    "gpe_count = entity_lab(word_counts['text_sentence'], 'GPE')\n",
    "language_count = entity_lab(word_counts['text_sentence'], 'LANGUAGE')\n",
    "loc_count = entity_lab(word_counts['text_sentence'], 'LOC')\n",
    "norp_count = entity_lab(word_counts['text_sentence'], 'NORP')\n",
    "ordinal_count = entity_lab(word_counts['text_sentence'], 'ORDINAL')\n",
    "org_count = entity_lab(word_counts['text_sentence'], 'ORG')\n",
    "person_count = entity_lab(word_counts['text_sentence'], 'PERSON')\n",
    "product_count = entity_lab(word_counts['text_sentence'], 'PRODUCT')\n",
    "quantity_count = entity_lab(word_counts['text_sentence'], 'QUANTITY')\n",
    "time_count = entity_lab(word_counts['text_sentence'], 'TIME')\n",
    "work_of_art_count = entity_lab(word_counts['text_sentence'], 'WORK_OF_ART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = word_counts\n",
    "features['CARDINAL'] = cardinal_count\n",
    "features['DATE'] = date_count\n",
    "features['FAC'] = fac_count\n",
    "features['GPE'] = gpe_count\n",
    "features['LANGUAGE'] = language_count\n",
    "features['LOC'] = loc_count\n",
    "features['NORP'] = norp_count\n",
    "features['ORDINAL'] = ordinal_count\n",
    "features['ORG'] = org_count\n",
    "features['PERSON'] = person_count\n",
    "features['PRODUCT'] = product_count\n",
    "features['QUANTITY'] = quantity_count\n",
    "features['TIME'] = time_count\n",
    "features['WORK_OF_ART'] = work_of_art_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undoubtedly</th>\n",
       "      <th>shoe</th>\n",
       "      <th>comprehend</th>\n",
       "      <th>heart</th>\n",
       "      <th>insult</th>\n",
       "      <th>convenient</th>\n",
       "      <th>benwick</th>\n",
       "      <th>chimney</th>\n",
       "      <th>stolen</th>\n",
       "      <th>belmont</th>\n",
       "      <th>...</th>\n",
       "      <th>adp_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>cconj_count</th>\n",
       "      <th>det_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>part_count</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>avg_wrd_len</th>\n",
       "      <th>avg_sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>5.298246</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>5.272727</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 3029 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  undoubtedly shoe comprehend heart insult convenient benwick chimney stolen  \\\n",
       "0           0    0          0     0      0          0       0       0      0   \n",
       "1           0    0          0     0      0          0       0       0      0   \n",
       "\n",
       "  belmont     ...      adp_count adv_count cconj_count det_count noun_count  \\\n",
       "0       0     ...              8         3           6         5         12   \n",
       "1       0     ...              8         7           2         6          8   \n",
       "\n",
       "  part_count pron_count verb_count avg_wrd_len avg_sent_len  \n",
       "0          2          3         13    5.298246           57  \n",
       "1          1          4         11    5.272727           55  \n",
       "\n",
       "[2 rows x 3029 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Test the Accuracy of the New Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "Y = features['text_source']\n",
    "X = np.array(features.drop(['text_sentence','text_source'], 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3190, 3027) (3190,)\n",
      "Training set score: 0.962382445141\n",
      "\n",
      "Test set score: 0.910714285714\n"
     ]
    }
   ],
   "source": [
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.992163009404\n",
      "\n",
      "Test set score: 0.845394736842\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "from sklearn import ensemble\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8934169279\n",
      "\n",
      "Test set score: 0.857142857143\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.689028213166\n",
      "\n",
      "Test set score: 0.681860902256\n"
     ]
    }
   ],
   "source": [
    "# SVC with new features\n",
    "\n",
    "train = svc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', svc.score(X_train, y_train))\n",
    "print('\\nTest set score:', svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw:\n",
      " [The Tragedie of Hamlet by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo a\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the chosen text--Hamlet.\n",
    "\n",
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "# Print the first 100 characters of Alice in Wonderland.\n",
    "print('\\nRaw:\\n', hamlet[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw:\n",
      " Enter Barnardo and Francisco two Centinels. Barnardo. Who's there? Fran. Nay answer me: Stand & vnfo\n"
     ]
    }
   ],
   "source": [
    "hamlet = re.sub(r'Actus .*', '', hamlet)\n",
    "\n",
    "hamlet = text_cleaner(hamlet)\n",
    "print('\\nRaw:\\n', hamlet[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned text\n",
    "hamlet_doc = nlp(hamlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "hamlet_sents = [[sent, \"Shakespeare\"] for sent in hamlet_doc.sents]\n",
    "\n",
    "# Combine the sentences of Hamlet with those of Carroll and Austen in separate DataFames\n",
    "sentences_C_S = pd.DataFrame(alice_sents + hamlet_sents)\n",
    "\n",
    "sentences_A_S = pd.DataFrame(hamlet_sents + persuasion_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a bag of words for Hamlet\n",
    "hamlet_words = bag_of_words(hamlet_doc)\n",
    "\n",
    "# Make list of the common words between Hamlet & Alice and Hamlet & Persuasion\n",
    "common_words_C_S = set(alicewords + hamlet_words)\n",
    "\n",
    "common_words_A_S = set(hamlet_words + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n",
      "Processing row 6500\n"
     ]
    }
   ],
   "source": [
    "# Create our data frames with features. \n",
    "features_C_S = bow_features(sentences_C_S, common_words_C_S)\n",
    "\n",
    "features_A_S = bow_features(sentences_A_S, common_words_A_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4705\n",
      "6685\n"
     ]
    }
   ],
   "source": [
    "print(len(features_C_S))\n",
    "print(len(features_A_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ents_C_S = []\n",
    "for i in range (0, 4704):\n",
    "    sent_ents_C_S.append(nlp(features_C_S['text_sentence'][i].string).ents) \n",
    "    \n",
    "sent_ents_A_S = []\n",
    "for i in range (0, 6684):\n",
    "    sent_ents_A_S.append(nlp(features_A_S['text_sentence'][i].string).ents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_labels_C_S = []\n",
    "for i in range (0, 4704):\n",
    "    for entity in sent_ents_C_S[i]:\n",
    "        ent_labels_C_S.append(entity.label_)\n",
    "        \n",
    "ent_labels_A_S = []\n",
    "for i in range (0, 6684):\n",
    "    for entity in sent_ents_A_S[i]:\n",
    "        ent_labels_A_S.append(entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Labels for Carroll and Shakespeare Dataset:\n",
      " {'CARDINAL', 'LOC', 'LAW', 'ORDINAL', 'ORG', 'GPE', 'WORK_OF_ART', 'EVENT', 'FAC', 'NORP', 'PRODUCT', 'TIME', 'LANGUAGE', 'DATE', 'PERSON', 'QUANTITY'}\n"
     ]
    }
   ],
   "source": [
    "set_entlabels_C_S = set(ent_labels_C_S)\n",
    "print('Entity Labels for Carroll and Shakespeare Dataset:\\n {}'.format(set_entlabels_C_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Labels for Carroll and Shakespeare Dataset:\n",
      " {'CARDINAL', 'LANGUAGE', 'LOC', 'WORK_OF_ART', 'ORG', 'ORDINAL', 'EVENT', 'FAC', 'NORP', 'DATE', 'TIME', 'PRODUCT', 'QUANTITY', 'GPE', 'PERSON', 'LAW'}\n"
     ]
    }
   ],
   "source": [
    "set_entlabels_A_S = set(ent_labels_A_S)\n",
    "print('Entity Labels for Carroll and Shakespeare Dataset:\\n {}'.format(set_entlabels_A_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing CARDINALs\n",
      "doing CARDINALs\n",
      "doing DATEs\n",
      "doing DATEs\n",
      "doing FACs\n",
      "doing FACs\n",
      "doing GPEs\n",
      "doing GPEs\n",
      "doing LANGUAGEs\n",
      "doing LANGUAGEs\n",
      "doing LOCs\n",
      "doing LOCs\n",
      "doing NORPs\n",
      "doing NORPs\n",
      "doing ORDINALs\n",
      "doing ORDINALs\n",
      "doing ORGs\n",
      "doing ORGs\n",
      "doing PERSONs\n",
      "doing PERSONs\n",
      "doing PRODUCTs\n",
      "doing PRODUCTs\n",
      "doing QUANTITYs\n",
      "doing QUANTITYs\n",
      "doing TIMEs\n",
      "doing TIMEs\n",
      "doing WORK_OF_ARTs\n",
      "doing WORK_OF_ARTs\n",
      "doing LAWs\n",
      "doing LAWs\n",
      "doing EVENTs\n",
      "doing EVENTs\n"
     ]
    }
   ],
   "source": [
    "cardinal_count_C_S = entity_lab(features_C_S['text_sentence'], 'CARDINAL')\n",
    "cardinal_count_A_S = entity_lab(features_A_S['text_sentence'], 'CARDINAL')\n",
    "date_count_C_S = entity_lab(features_C_S['text_sentence'], 'DATE')\n",
    "date_count_A_S = entity_lab(features_A_S['text_sentence'], 'DATE')\n",
    "fac_count_C_S = entity_lab(features_C_S['text_sentence'], 'FAC')\n",
    "fac_count_A_S = entity_lab(features_A_S['text_sentence'], 'FAC')\n",
    "gpe_count_C_S = entity_lab(features_C_S['text_sentence'], 'GPE')\n",
    "gpe_count_A_S = entity_lab(features_A_S['text_sentence'], 'GPE')\n",
    "language_count_C_S = entity_lab(features_C_S['text_sentence'], 'LANGUAGE')\n",
    "language_count_A_S = entity_lab(features_A_S['text_sentence'], 'LANGUAGE')\n",
    "loc_count_C_S = entity_lab(features_C_S['text_sentence'], 'LOC')\n",
    "loc_count_A_S = entity_lab(features_A_S['text_sentence'], 'LOC')\n",
    "norp_count_C_S = entity_lab(features_C_S['text_sentence'], 'NORP')\n",
    "norp_count_A_S = entity_lab(features_A_S['text_sentence'], 'NORP')\n",
    "ordinal_count_C_S = entity_lab(features_C_S['text_sentence'], 'ORDINAL')\n",
    "ordinal_count_A_S = entity_lab(features_A_S['text_sentence'], 'ORDINAL')\n",
    "org_count_C_S = entity_lab(features_C_S['text_sentence'], 'ORG')\n",
    "org_count_A_S = entity_lab(features_A_S['text_sentence'], 'ORG')\n",
    "person_count_C_S = entity_lab(features_C_S['text_sentence'], 'PERSON')\n",
    "person_count_A_S = entity_lab(features_A_S['text_sentence'], 'PERSON')\n",
    "product_count_C_S = entity_lab(features_C_S['text_sentence'], 'PRODUCT')\n",
    "product_count_A_S = entity_lab(features_A_S['text_sentence'], 'PRODUCT')\n",
    "quantity_count_C_S = entity_lab(features_C_S['text_sentence'], 'QUANTITY')\n",
    "quantity_count_A_S = entity_lab(features_A_S['text_sentence'], 'QUANTITY')\n",
    "time_count_C_S = entity_lab(features_C_S['text_sentence'], 'TIME')\n",
    "time_count_A_S = entity_lab(features_A_S['text_sentence'], 'TIME')\n",
    "work_of_art_count_C_S = entity_lab(features_C_S['text_sentence'], 'WORK_OF_ART')\n",
    "work_of_art_count_A_S = entity_lab(features_A_S['text_sentence'], 'WORK_OF_ART')\n",
    "law_count_C_S = entity_lab(features_C_S['text_sentence'], 'LAW')\n",
    "law_count_A_S = entity_lab(features_A_S['text_sentence'], 'LAW')\n",
    "event_count_C_S = entity_lab(features_C_S['text_sentence'], 'EVENT')\n",
    "event_count_A_S = entity_lab(features_A_S['text_sentence'], 'EVENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_word_len_C_S = [len(features_C_S['text_sentence'][i].string) / \n",
    "            len(features_C_S['text_sentence'][i].string.split())\n",
    "                for i in range (len(features_C_S['text_sentence']))]\n",
    "\n",
    "avg_word_len_A_S = [len(features_A_S['text_sentence'][i].string) / \n",
    "            len(features_A_S['text_sentence'][i].string.split())\n",
    "                for i in range (len(features_A_S['text_sentence']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_C_S['avg_wrd_len'] = avg_word_len_C_S\n",
    "\n",
    "features_A_S['avg_wrd_len'] = avg_word_len_A_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrdspersent_C_S = [len(features_C_S['text_sentence'][i].string.split()) \n",
    "               for i in range (len(features_C_S['text_sentence']))]\n",
    "\n",
    "wrdspersent_A_S = [len(features_A_S['text_sentence'][i].string.split()) \n",
    "               for i in range (len(features_A_S['text_sentence']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_C_S['avg_sent_len'] = wrdspersent_C_S\n",
    "\n",
    "features_A_S['avg_sent_len'] = wrdspersent_A_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cou'nant</th>\n",
       "      <th>shoe</th>\n",
       "      <th>ambitious</th>\n",
       "      <th>heart</th>\n",
       "      <th>insult</th>\n",
       "      <th>most</th>\n",
       "      <th>chimney</th>\n",
       "      <th>stolen</th>\n",
       "      <th>contagion</th>\n",
       "      <th>peepe</th>\n",
       "      <th>...</th>\n",
       "      <th>grone</th>\n",
       "      <th>forfeite</th>\n",
       "      <th>poleak</th>\n",
       "      <th>weepe</th>\n",
       "      <th>e'ene</th>\n",
       "      <th>husband</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "      <th>avg_wrd_len</th>\n",
       "      <th>avg_sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>5.298246</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>5.272727</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>4.862069</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3291 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  cou'nant shoe ambitious heart insult most chimney stolen contagion peepe  \\\n",
       "0        0    0         0     0      0    0       0      0         0     0   \n",
       "1        0    0         0     0      0    0       0      0         0     0   \n",
       "2        0    0         0     0      0    0       0      0         0     0   \n",
       "\n",
       "      ...      grone forfeite poleak weepe e'ene husband  \\\n",
       "0     ...          0        0      0     0     0       0   \n",
       "1     ...          0        0      0     0     0       0   \n",
       "2     ...          0        0      0     0     0       0   \n",
       "\n",
       "                                       text_sentence text_source avg_wrd_len  \\\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll    5.298246   \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll    5.272727   \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll    4.862069   \n",
       "\n",
       "  avg_sent_len  \n",
       "0           57  \n",
       "1           55  \n",
       "2           29  \n",
       "\n",
       "[3 rows x 3291 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_C_S.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undoubtedly</th>\n",
       "      <th>cou'nant</th>\n",
       "      <th>ambitious</th>\n",
       "      <th>heart</th>\n",
       "      <th>comprehend</th>\n",
       "      <th>convenient</th>\n",
       "      <th>insult</th>\n",
       "      <th>most</th>\n",
       "      <th>benwick</th>\n",
       "      <th>contagion</th>\n",
       "      <th>...</th>\n",
       "      <th>forfeite</th>\n",
       "      <th>poleak</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>weepe</th>\n",
       "      <th>e'ene</th>\n",
       "      <th>husband</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "      <th>avg_wrd_len</th>\n",
       "      <th>avg_sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Enter, Barnardo, and, Francisco, two, Centine...</td>\n",
       "      <td>Shakespeare</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Barnardo, .)</td>\n",
       "      <td>Shakespeare</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Who, 's, there, ?)</td>\n",
       "      <td>Shakespeare</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3426 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  undoubtedly cou'nant ambitious heart comprehend convenient insult most  \\\n",
       "0           0        0         0     0          0          0      0    0   \n",
       "1           0        0         0     0          0          0      0    0   \n",
       "2           0        0         0     0          0          0      0    0   \n",
       "\n",
       "  benwick contagion     ...      forfeite poleak afternoon weepe e'ene  \\\n",
       "0       0         0     ...             0      0         0     0     0   \n",
       "1       0         0     ...             0      0         0     0     0   \n",
       "2       0         0     ...             0      0         0     0     0   \n",
       "\n",
       "  husband                                      text_sentence  text_source  \\\n",
       "0       0  (Enter, Barnardo, and, Francisco, two, Centine...  Shakespeare   \n",
       "1       0                                      (Barnardo, .)  Shakespeare   \n",
       "2       0                                (Who, 's, there, ?)  Shakespeare   \n",
       "\n",
       "  avg_wrd_len avg_sent_len  \n",
       "0    7.333333            6  \n",
       "1   10.000000            1  \n",
       "2    6.500000            2  \n",
       "\n",
       "[3 rows x 3426 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_A_S.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = features_C_S['text_source']\n",
    "X = np.array(features_C_S.drop(['text_sentence','text_source'], 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2823, 3289) (2823,)\n",
      "Training set score: 0.964222458378\n",
      "\n",
      "Test set score: 0.917640807651\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression: C_S\n",
    "\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.995040736805\n",
      "\n",
      "Test set score: 0.88788522848\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier: C_S\n",
    "\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.879206517889\n",
      "\n",
      "Test set score: 0.848565356004\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier: C_S\n",
    "\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.669854764435\n",
      "\n",
      "Test set score: 0.675345377258\n"
     ]
    }
   ],
   "source": [
    "# SVCP: C_S\n",
    "\n",
    "train = svc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', svc.score(X_train, y_train))\n",
    "print('\\nTest set score:', svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
